[0m[[0m[31merror[0m] [0m[0morg.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/subhrajit/sparkProjects/data/log.txt[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:285)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:228)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:313)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:200)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:35)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:253)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD$$anonfun$partitions$2.apply(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.Option.getOrElse(Option.scala:121)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.partitions(RDD.scala:251)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.Partitioner$$anonfun$4.apply(Partitioner.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.Partitioner$$anonfun$4.apply(Partitioner.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.foreach(List.scala:392)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.TraversableLike$class.map(TraversableLike.scala:234)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.collection.immutable.List.map(List.scala:296)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.Partitioner$.defaultPartitioner(Partitioner.scala:75)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:326)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions$$anonfun$reduceByKey$3.apply(PairRDDFunctions.scala:326)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.RDD.withScope(RDD.scala:363)[0m
[0m[[0m[31merror[0m] [0m[0m	at org.apache.spark.rdd.PairRDDFunctions.reduceByKey(PairRDDFunctions.scala:325)[0m
[0m[[0m[31merror[0m] [0m[0m	at wordcount$.main(1.scala:32)[0m
[0m[[0m[31merror[0m] [0m[0m	at wordcount.main(1.scala)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.reflect.Method.invoke(Method.java:566)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.invokeMain(Run.scala:143)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.execute$1(Run.scala:93)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.$anonfun$runWithLoader$5(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run$.executeSuccess(Run.scala:186)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Run.runWithLoader(Run.scala:120)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$bgRunTask$6(Defaults.scala:1983)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.Defaults$.$anonfun$termWrapper$2(Defaults.scala:1922)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)[0m
[0m[[0m[31merror[0m] [0m[0m	at scala.util.Try$.apply(Try.scala:213)[0m
[0m[[0m[31merror[0m] [0m[0m	at sbt.internal.BackgroundThreadPool$BackgroundRunnable.run(DefaultBackgroundJobService.scala:369)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)[0m
[0m[[0m[31merror[0m] [0m[0m	at java.base/java.lang.Thread.run(Thread.java:829)[0m
[0m[[0m[31merror[0m] [0m[0m(Compile / [31mrun[0m) org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/home/subhrajit/sparkProjects/data/log.txt[0m
